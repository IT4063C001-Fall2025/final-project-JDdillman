{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# {The Rising Impact of Data Breaches in the U.S.}üìù\n",
    "\n",
    "![Banner](./assets/banner.jpeg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic\n",
    "*What problem are you (or your stakeholder) trying to address?*\n",
    "üìù <!-- Answer Below -->\n",
    "I want to talk about cybersecurity and data privacy, specifically analyzing patterns in data \n",
    "breaches in the United States. Every year, millions of people are affected by compromised personal \n",
    "data, which can result in identity theft, financial loss, and a decline in faith in businesses and \n",
    "technology. The rise of cloud platforms, artificial intelligence, and digitalservices has led to a \n",
    "rise in the collection of sensitive data, but laws and safeguards are frequently lagging behind. \n",
    "This makes the topic important today. Analyzing breach trends can help us identify weak points and \n",
    "improve data security for both individuals and corporations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Question\n",
    "*What specific question are you seeking to answer with this project?*\n",
    "*This is not the same as the questions you ask to limit the scope of the project.*\n",
    "üìù <!-- Answer Below -->\n",
    "- How has breach frequency changed over time in the U.S.?\n",
    "- Which industries are most frequently targeted?\n",
    "- What types of data (PII, PHI, financial) are most often exposed?\n",
    "- Can a model predict whether a breach contains PII/PHI based on the breach summary?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would an answer look like?\n",
    "*What is your hypothesized answer to your question?*\n",
    "üìù <!-- Answer Below -->\n",
    "- The number of reported breaches increases over time (more reporting & attacks)\n",
    "- Finance and healthcare will be among the most-targeted industries\n",
    "- PII and health data will appear frequently\n",
    "- A text classifier can detect PII mentions in breach summaries with reasonable accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "*What 3 data sources have you identified for this project?*\n",
    "*How are you going to relate these datasets?*\n",
    "üìù <!-- Answer Below -->\n",
    "Data Sources Used:\n",
    "1. Dataset 1 (Df_1.csv)\n",
    "2. Dataset 2 (breach_report.csv)\n",
    "3. Dataset 3 (cyber_security_breaches.csv)\n",
    "\n",
    "By finding the common fields that show up in all of the datasetssuch as the breach date, organization name, industry sector, breach type, and the quantity of documents exposed I am connecting them. These shared fields enable me to combine each dataset into a single, cohesive dataset, despite the fact that they each concentrate on distinct facets of cybersecurity incidents. Even if datasets cannot be directly combined, I can still conduct independent analyses and use the findings to bolster the main conclusions. I can analyze trends across industries, time periods, and breach characteristics by merging the data in this way, giving me a broader and more comprehensive picture of U.S. data breaches.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach and Analysis\n",
    "*What is your approach to answering your project question?*\n",
    "*How will you use the identified data to answer your project question?*\n",
    "My method involves cleaning and standardizing all of the breach data so that it can be studied uniformly. After that, I examine the data to find trends, outliers, and connections between factors like year, industry, type of breach, and records exposed. After that, I make visualizations that show trends, such as how the frequency of breaches varies over time or which industries are most impacted. In order to determine whether breach severity can be anticipated using the dataset's attributes, I also employ machine-learning algorithms. Overall, I can address my project issue and demonstrate how data breaches have changed and what aspects are most important in security events by combining the datasets, investigating the trends, creating visuals, and using ML models.\n",
    "\n",
    "I'll measure the frequency of breaches, the industries most affected, and the kinds of data exposed using the aggregated breach datasets. I can spot trends like rising breach frequency over time or industries with the highest occurrence rates by examining the dates, industry classifications, breach kinds, and record counts. While summary data can draw attention to significant changes or anomalous occurrences, visualizations will aid in the clear revelation of these trends. In order to determine whether particular characteristics, such as industry or breach kind, have a significant impact, I will also utilize the data to train machine-learning models that aim to forecast breach severity. The datasets collectively offer the proof required to address my project's questions and bolster significant findings regarding cybersecurity and data-privacy threats in the US.\n",
    "üìù <!-- Start Discussing the project here; you can add as many code cells as you need -->\n",
    "\n",
    "\n",
    "\n",
    "## Data Cleaning Pipeline (summary)\n",
    "- Normalize column names (lowercase, underscores)  \n",
    "- Parse and standardize date/year fields  \n",
    "- Impute missing or malformed numeric values if needed (median imputation used for numeric summaries)  \n",
    "- Drop duplicate rows  \n",
    "- Create `pii_exposed` label via keyword detection in `summary`  \n",
    "- Save a cleaned dataset for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries Cell\n",
    "Summary:\n",
    "This section offers openness into the source of the data and documents the dataset used in the project. The primary dataset, Cyber Security Breaches.csv, comprises breach narratives, dates, organizations, locations, and additional metadata. Clearly identifying the data sources guarantees reproducibility and lays the groundwork for the subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import joblib\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (make sure Cyber Security Breaches.csv is in repo root or data/)\n",
    "DATA_PATH = \"Cyber Security Breaches.csv\"\n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "df.info()\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "# Missing values and duplicates\n",
    "print(\"Missing values per column:\\n\", df.isnull().sum())\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure summary column exists and is string\n",
    "df['summary'] = df.get('summary', '').astype(str).fillna('')\n",
    "\n",
    "# Parse date-like columns to create year\n",
    "date_cols = [c for c in df.columns if 'date' in c or 'breach' in c and 'date' in c]\n",
    "# Prefer common fields\n",
    "if 'date_of_breach' in df.columns:\n",
    "    df['date_of_breach'] = pd.to_datetime(df['date_of_breach'], errors='coerce')\n",
    "    df['year'] = df['date_of_breach'].dt.year\n",
    "elif 'breach_start' in df.columns:\n",
    "    df['breach_start'] = pd.to_datetime(df['breach_start'], errors='coerce')\n",
    "    df['year'] = df['breach_start'].dt.year\n",
    "elif 'year' in df.columns:\n",
    "    df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "else:\n",
    "    df['year'] = pd.NA\n",
    "\n",
    "# Standardize columns we may use (create if missing)\n",
    "for col in ['industry','breach_type','state','organization']:\n",
    "    if col not in df.columns:\n",
    "        df[col] = pd.NA\n",
    "\n",
    "# Drop exact duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Attempt to normalize any numeric 'records' columns if present\n",
    "possible_records = [c for c in df.columns if 'record' in c or 'expos' in c or 'comprom' in c]\n",
    "if possible_records:\n",
    "    rc = possible_records[0]\n",
    "    df['records_exposed'] = pd.to_numeric(df[rc].astype(str).str.replace(r'[^0-9]', '', regex=True), errors='coerce')\n",
    "else:\n",
    "    df['records_exposed'] = pd.NA\n",
    "\n",
    "# Print counts after cleaning\n",
    "print(\"After cleaning shape:\", df.shape)\n",
    "print(\"Year nulls:\", df['year'].isnull().sum(), \"Records_exposed nulls:\", df['records_exposed'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a binary target: whether the summary mentions PII / PHI indicators\n",
    "pii_terms = [\n",
    "    r'\\bssn\\b', 'social security', 'patient', 'protected health', r'\\bphi\\b',\n",
    "    'health information', 'personal information', 'credit card', 'card number',\n",
    "    'financial account', 'date of birth', r'\\bdob\\b', 'driver license', 'passport',\n",
    "    'medical record', 'email', 'address'\n",
    "]\n",
    "pattern = re.compile('|'.join(pii_terms), flags=re.IGNORECASE)\n",
    "\n",
    "df['pii_exposed'] = df['summary'].apply(lambda s: 1 if pattern.search(str(s)) else 0)\n",
    "print(\"PII label counts:\\n\", df['pii_exposed'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ‚Äî Breaches per year (line)\n",
    "if df['year'].notna().any():\n",
    "    year_counts = df['year'].dropna().astype(int).value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    plt.plot(year_counts.index, year_counts.values, marker='o')\n",
    "    plt.title(\"Number of Reported Breaches per Year\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count of Breaches\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 ‚Äî Top industries by count\n",
    "if 'industry' in df.columns:\n",
    "    top_ind = df['industry'].fillna('Unknown').value_counts().head(10)\n",
    "    plt.figure()\n",
    "    top_ind.plot(kind='barh')\n",
    "    plt.title(\"Top 10 Industries by Number of Breaches\")\n",
    "    plt.xlabel(\"Number of Breaches\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 ‚Äî PII presence pie\n",
    "vals = df['pii_exposed'].value_counts().sort_index()\n",
    "labels = ['No PII', 'PII Mentioned']\n",
    "plt.figure()\n",
    "plt.pie(vals.values, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "plt.title(\"Proportion of Breaches Mentioning PII/PHI in Summary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 ‚Äî summary length histogram\n",
    "lengths = df['summary'].str.len()\n",
    "plt.figure()\n",
    "plt.hist(lengths.dropna(), bins=30)\n",
    "plt.title(\"Distribution of Summary Length (characters)\")\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 ‚Äî Year vs summary length scatter (log y)\n",
    "if df['year'].notna().any():\n",
    "    plt.figure()\n",
    "    idx = df['year'].dropna().index\n",
    "    plt.scatter(df.loc[idx, 'year'].astype(int), df.loc[idx, 'summary'].str.len(), alpha=0.4)\n",
    "    plt.yscale('log')\n",
    "    plt.title(\"Summary Length vs Year (log scale)\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Summary Length (log scale)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 ‚Äî Top words in PII summaries\n",
    "def top_n_words(texts, n=15):\n",
    "    words = Counter()\n",
    "    for t in texts:\n",
    "        toks = re.findall(r'\\b[a-zA-Z]{3,}\\b', t.lower())\n",
    "        words.update(toks)\n",
    "    return words.most_common(n)\n",
    "\n",
    "pii_top = top_n_words(df[df['pii_exposed']==1]['summary'].astype(str).tolist(), n=20)\n",
    "if pii_top:\n",
    "    words, counts = zip(*pii_top)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(\"Top Words in PII-Related Summaries\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML (text classification)\n",
    "X = df['summary'].astype(str)\n",
    "y = df['pii_exposed']\n",
    "\n",
    "# Train / test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=123, stratify=y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', max_features=5000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=123))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['No PII','PII'])\n",
    "plt.figure()\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - PII detection\")\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - PII detection\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top positive predictive terms\n",
    "clf = pipeline.named_steps['clf']\n",
    "tfidf = pipeline.named_steps['tfidf']\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "coefs = clf.coef_[0]\n",
    "top_pos_idx = np.argsort(coefs)[-20:][::-1]\n",
    "\n",
    "print(\"Top features predictive of PII (positive coefficients):\")\n",
    "for i in top_pos_idx[:20]:\n",
    "    print(f\"{feature_names[i]}: {coefs[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained pipeline + cleaned CSV\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(pipeline, \"models/pii_detector_tfidf_logreg.joblib\")\n",
    "cleaned_csv = \"cleaned_cyber_breaches.csv\"\n",
    "df.to_csv(cleaned_csv, index=False)\n",
    "print(\"Saved model to models/pii_detector_tfidf_logreg.joblib\")\n",
    "print(\"Saved cleaned data to\", cleaned_csv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources and References\n",
    "*What resources and references have you used for this project?*\n",
    "üìù <!-- Answer Below -->\n",
    "https://www.nist.gov/cyberframework\n",
    "https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf\n",
    "https://privacyrights.org/\n",
    "https://www.idtheftcenter.org/\n",
    "https://www.cisa.gov/\n",
    "https://pandas.pydata.org/docs/\n",
    "https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf\n",
    "https://privacyrights.org/data-breaches\n",
    "https://www.idtheftcenter.org/\n",
    "https://jupyter-notebook.readthedocs.io/en/stable/\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "https://pandas.pydata.org/docs/\n",
    "https://numpy.org/doc/\n",
    "https://matplotlib.org/stable/users/index.html\n",
    "https://scikit-learn.org/stable/\n",
    "https://www.nltk.org/\n",
    "https://docs.python.org/3/library/re.html\n",
    "https://www.verizon.com/business/resources/reports/dbir/\n",
    "https://ocrportal.hhs.gov/ocr/breach/breach_report.jsf?utm_source=chatgpt.com\n",
    "https://www.idtheftcenter.org/breach-alert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook source.ipynb to python\n",
      "[NbConvertApp] Writing 1271 bytes to source.py\n"
     ]
    }
   ],
   "source": [
    "# ‚ö†Ô∏è Make sure you run this cell at the end of your notebook before every submission!\n",
    "!jupyter nbconvert --to python source.ipynb\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
